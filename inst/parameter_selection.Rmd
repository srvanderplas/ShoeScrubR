---
title: "Automatic Parameter Selection"
author: "Susan Vanderplas"
date: "9/27/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, dpi = 300)
library(EBImage)
library(ShoeScrubR)
library(tidyverse)
```

```{r data-setup}
lss_dir <- "/lss/research/csafe-shoeprints/ShoeImagingPermanent"

# For a bunch of images...
full_imglist <- list.files("/lss/research/csafe-shoeprints/ShoeImagingPermanent/",
                           pattern = "0[01]\\d{4}[RL]_\\d{8}_5_1_1", full.names = T)
dir <- tempdir()

file.copy(full_imglist, file.path(dir, basename(full_imglist)))
imglist <- file.path(dir, basename(full_imglist))

shoe_info <- read.csv("~/Projects/CSAFE/2018_Longitudinal_Shoe_Project/Clean_Data/shoe-info.csv") %>%
  filter(ShoeID %in% as.numeric(str_sub(basename(imglist), 1, 3))) %>%
  select(ShoeID, Brand, Size) %>%
  mutate(Size = str_remove(Size, "[ MW]") %>% parse_number()) %>%
  crossing(tibble(Mask_foot = c("R", "L"), Shoe_foot = c("L", "R"))) %>%
  mutate(mask = purrr::pmap(list(Brand, Size, Mask_foot, ppi = 300), shoe_mask))

scan_info <- tibble(
  file = imglist,
  ShoeID = str_extract(basename(file), "^\\d{3}") %>% parse_integer(),
  Shoe_foot = str_extract(basename(file), "\\d{6}[RL]") %>% str_remove_all("\\d"),
  date = str_extract(basename(file), "\\d{8}") %>% parse_date(format = "%Y%m%d")
) %>%
  left_join(select(shoe_info, ShoeID, Brand, Size, Shoe_foot)) %>%
  group_by(Shoe_foot, Brand) %>%
  sample_n(5) %>%
  ungroup() %>%
  group_by(ShoeID, Shoe_foot) %>%
  arrange(desc(date)) %>%
  filter(row_number() == 1) %>%
  ungroup() %>%
  mutate(
    img = purrr::map(file, EBImage::readImage, all = F),
    img = purrr::map(img, EBImage::channel, "luminance"),
    im_dim = purrr::map(img, dim)
  )

```

## Parameters in the Rough Alignment algorithm

Image Exaggeration

1. Gaussian blur
    - kernel size
    - kernel shape
    - sigma
2. Threshold for binarization
3. Opening diameter
4. Closing diameter

## Selection of Binarization Threshold

```{r}
par(mfrow = c(1, 2))
plot(scan_info$img[[1]])
hist(scan_info$img[[1]])

```
- Use EM Algorithm - each image is a mixture of signal and noise pixels, with different intensity distributions.

```{r}
em_thresh <- function(img, scale = 10, ngroups = 3) {
  imsmall <- img_resize(img, w = floor(dim(img)[1]/scale), h = floor(dim(img)[2]/scale))
  em <- mixtools::normalmixEM(imsmall, k = ngroups)
  values <- cbind(em$x, em$posterior)
  # values <- values[order(values[,1]),]
  # values[values[,2]*100 < values[,3], ]
  
  mean_idx <- order(em$mu, decreasing = F)
  
  # get probs for full img
  img_dens_functions <- lapply(mean_idx, function(x) 0*img + dnorm(img, mean = em$mu[x], sd = em$sigma[x]))
  
  img_dens_total <- abind::abind(img_dens_functions, along = 3) %>%
    apply(c(1, 2), sum)
  
  img_dens_ratio <- lapply(img_dens_functions, function(x) as.Image(x/(img_dens_total - x)))
  
  return(list(img_densities = img_dens_functions, img_ratios = img_dens_ratio, em = em))
}

emt <- em_thresh(scan_info$img[[1]])
Image(abind::abind(emt$img_ratios, along = 3)) %>% plot(all = T, nx = 3)
```

The first image shows pixels(in white) identified as most likely providing signal data. The third image shows pixels identified as most likely belonging to the background. The second image shows intermediate pixels, which are not necessarily consistent with background or signal. 

One primary advantage of using only the first image is that it is nearly discrete - the values are the likelihood ratio of belonging to signal vs. other (background + intermediate), and are almost completely separated between infinite and 0. 

This allows us to get away from selecting a single threshold value in favor of computing an implicit automatic threshold via the EM algorithm. 


## Diameter selection

Idea - use `rle` to count runs in rows and columns to get an *approximate* idea of which parameters to use for tuning. E.g. using binarized image, runs of 0 will be either relatively short (adidas stripes, between Nike blobs) or relatively long (outside of the shoe). 

```{r}
bin_img <- normalize(emt$img_ratios[[1]])
bin_img[is.nan(bin_img)] <- 1

bin_img[1000,] %>% as.numeric() %>% rle() %>% do.call("bind_cols", .) %>% filter(values == 1) %>% `[[`(1) %>% median()

col_runs <- apply(bin_img, 1, function(x) rle(x) %>% do.call("bind_cols", .)) %>%
  purrr::map2_df(., 1:length(.), ~dplyr::mutate(.x, idx = .y, type = "col", prop_lengths = lengths/dim(bin_img)[2]))

row_runs <- apply(bin_img, 2, function(x) rle(x) %>% do.call("bind_cols", .)) %>%
  purrr::map2_df(., 1:length(.), ~dplyr::mutate(.x, idx = .y, type = "row", prop_lengths = lengths/dim(bin_img)[1]))

runs <- bind_rows(col_runs, row_runs) %>%
  group_by(type) %>%
  mutate(pos = idx/max(idx)) %>%
  ungroup()
```

In this plot, there is a cluster of 0-runs of length 100(ish) and 250(ish), while the 1-runs are at most 60 pixels long, and usually closer to 10-20. The better question is how do we "see" that automatically?
```{r}
ggplot(data = runs, aes(y = lengths, x = pos, fill = factor(values))) + geom_jitter(alpha = .05) + facet_wrap(~type + values, scales = 'free')
```

```{r}
# get middle 2/3 of values
trim_1_3 <- function(x, y) {
  cuts <- quantile(x, c(.16, .84))
  y[x > cuts[1] & x < cuts[2]]
}

col_runs_trim_0 <- filter(tmp, values == 0) %>%
  filter(col > quantile(col, .16) & col < quantile(col, .84))
```

```{r}
bin_img_clean <- bin_img %>%
  erode(makeBrush(size = 1)) %>%
  dilate(makeBrush(size = 3, shape = "line", angle = 0)) %>%
  dilate(makeBrush(size = 3, shape = "line", angle = 45)) %>%
  dilate(matrix(c(0, 0, 1, 0, 1, 0, 1, 0, 0))) %>%
  erode(makeBrush(size = 3)) %>%
  dilate(makeBrush(size = 9, shape = "disc"))
plot(bin_img_clean)
```